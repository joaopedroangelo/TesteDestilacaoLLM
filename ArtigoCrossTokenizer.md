# Towards Cross-Tokenizer Distillation

> Artigo: https://arxiv.org/pdf/2402.12030

---
## Visão Geral

O artigo "Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs" aborda um método de destilação de modelos de linguagem (LLMs) que utiliza um mecanismo chamado "Universal Logit Distillation Loss".

O foco é permitir a destilação entre modelos que utilizam tokenizadores diferentes, o que geralmente é um desafio devido às discrepâncias na segmentação de tokens. O método proposto busca alinhar as distribuições de logits de maneira universal, facilitando o aprendizado do aluno independentemente do tokenizador utilizado.

---
